/**
 * LoRA Adapter Configuration Generator
 *
 * Generates PEFT adapter configuration and training scripts.
 */

import type { LoRAAdapterConfig } from './types.js';

/**
 * Default LoRA configurations for different model sizes
 */
const DEFAULT_CONFIGS: Record<string, Partial<LoRAAdapterConfig>> = {
  small: {
    r: 8,
    lora_alpha: 16,
    lora_dropout: 0.05,
  },
  medium: {
    r: 16,
    lora_alpha: 32,
    lora_dropout: 0.05,
  },
  large: {
    r: 32,
    lora_alpha: 64,
    lora_dropout: 0.1,
  },
};

/**
 * Target modules for popular model architectures
 */
const TARGET_MODULES: Record<string, string[]> = {
  llama: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
  mistral: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
  gpt2: ['c_attn', 'c_proj', 'c_fc'],
  bloom: ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],
  t5: ['q', 'v', 'k', 'o', 'wi', 'wo'],
  default: ['q_proj', 'v_proj'],
};

/**
 * Generate LoRA adapter configuration
 */
export function generateAdapterConfig(options?: {
  modelType?: string;
  size?: 'small' | 'medium' | 'large';
  rank?: number;
  alpha?: number;
  dropout?: number;
  targetModules?: string[];
}): LoRAAdapterConfig {
  const size = options?.size || 'medium';
  const defaults = DEFAULT_CONFIGS[size] || DEFAULT_CONFIGS.medium;
  const modelType = options?.modelType || 'default';

  return {
    r: options?.rank ?? defaults?.r ?? 16,
    lora_alpha: options?.alpha ?? defaults?.lora_alpha ?? 32,
    lora_dropout: options?.dropout ?? defaults?.lora_dropout ?? 0.05,
    target_modules: options?.targetModules ??
      TARGET_MODULES[modelType] ??
      TARGET_MODULES.default ?? ['q_proj', 'v_proj'],
    bias: 'none',
    task_type: 'CAUSAL_LM',
    inference_mode: false,
  };
}

/**
 * Generate adapter config JSON file content
 */
export function generateAdapterConfigJSON(config: LoRAAdapterConfig): string {
  return JSON.stringify(config, null, 2);
}

/**
 * Generate training script stub
 */
export function generateTrainingScript(options?: {
  modelName?: string;
  datasetPath?: string;
  outputDir?: string;
  adapterConfig?: LoRAAdapterConfig;
}): string {
  const modelName = options?.modelName || 'meta-llama/Llama-2-7b-hf';
  const datasetPath = options?.datasetPath || './train.jsonl';
  const outputDir = options?.outputDir || './lora-output';
  const config = options?.adapterConfig || generateAdapterConfig();

  return `#!/usr/bin/env python3
"""
LoRA Fine-tuning Script

Generated by Agent Memory LoRA Export.
Customize this script for your specific training requirements.
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# =============================================================================
# CONFIGURATION
# =============================================================================

MODEL_NAME = "${modelName}"
DATASET_PATH = "${datasetPath}"
OUTPUT_DIR = "${outputDir}"

# LoRA configuration
LORA_CONFIG = LoraConfig(
    r=${config.r},
    lora_alpha=${config.lora_alpha},
    lora_dropout=${config.lora_dropout},
    target_modules=${JSON.stringify(config.target_modules)},
    bias="${config.bias}",
    task_type="${config.task_type}",
)

# Training arguments
TRAINING_ARGS = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    warmup_steps=100,
    lr_scheduler_type="cosine",
)

# =============================================================================
# MAIN
# =============================================================================

def main():
    print("Loading tokenizer and model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # Prepare model for training
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, LORA_CONFIG)

    print(f"Model parameters: {model.print_trainable_parameters()}")

    # Load dataset
    print(f"Loading dataset from {DATASET_PATH}...")
    dataset = load_dataset('json', data_files={
        'train': DATASET_PATH,
        'eval': DATASET_PATH.replace('train', 'eval'),
    })

    # Tokenize dataset
    def tokenize_function(examples):
        # Customize this based on your data format
        texts = []
        for instruction, input_text, output in zip(
            examples['instruction'],
            examples.get('input', [''] * len(examples['instruction'])),
            examples['output']
        ):
            prompt = f"### Instruction:\\n{instruction}\\n"
            if input_text:
                prompt += f"### Input:\\n{input_text}\\n"
            prompt += f"### Response:\\n{output}"
            texts.append(prompt)

        return tokenizer(
            texts,
            truncation=True,
            max_length=512,
            padding="max_length",
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset['train'].column_names,
    )

    # Train
    print("Starting training...")
    trainer = Trainer(
        model=model,
        args=TRAINING_ARGS,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['eval'],
    )

    trainer.train()

    # Save final model
    print(f"Saving model to {OUTPUT_DIR}...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print("Training complete!")

if __name__ == "__main__":
    main()
`;
}

/**
 * Generate requirements.txt for training environment
 */
export function generateRequirementsTxt(): string {
  return `# Python dependencies for LoRA fine-tuning
torch>=2.0.0
transformers>=4.35.0
peft>=0.6.0
datasets>=2.14.0
accelerate>=0.24.0
bitsandbytes>=0.41.0
sentencepiece>=0.1.99
protobuf>=3.20.0
`;
}

/**
 * Generate dataset info YAML
 */
export function generateDatasetInfo(stats: {
  totalExamples: number;
  trainExamples: number;
  evalExamples: number;
  format: string;
}): string {
  return `# Dataset Information

dataset_name: Agent Memory LoRA Training Dataset
version: 1.0.0
format: ${stats.format}

statistics:
  total_examples: ${stats.totalExamples}
  train_examples: ${stats.trainExamples}
  eval_examples: ${stats.evalExamples}
  split_ratio: ${(stats.evalExamples / stats.totalExamples).toFixed(2)}

description: |
  This dataset was automatically generated from Agent Memory guidelines
  for fine-tuning language models using LoRA (Low-Rank Adaptation).

  The dataset contains examples derived from project-specific guidelines,
  best practices, and coding standards stored in the Agent Memory system.

usage:
  - Load the dataset using the Hugging Face datasets library
  - Apply LoRA adapter using the provided adapter_config.json
  - Fine-tune using the training script or your custom training loop

citation: |
  Generated by Agent Memory - https://github.com/cyanheads/agent-memory
`;
}
