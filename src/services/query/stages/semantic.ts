/**
 * Semantic Stage
 *
 * Performs vector similarity search when the strategy is 'semantic' or 'hybrid'.
 * Generates query embeddings and populates semanticScores for use in scoring.
 *
 * Uses injected dependencies for embedding service and vector service.
 *
 * Pipeline Context Extensions:
 * - Requires `searchStrategy` in PipelineContext (from strategy stage)
 * - Adds `queryEmbedding?: number[]` to PipelineContext for downstream use
 * - Populates `semanticScores` map with entry IDs and similarity scores
 *
 * NOTE: Non-null assertions used for Map access after has() checks in cache operations.
 */

/* eslint-disable @typescript-eslint/no-non-null-assertion */

import type { PipelineContext } from '../pipeline.js';
import type { SearchStrategy } from './strategy.js';
import { AgentMemoryError, ErrorCodes } from '../../../core/errors.js';
import { LRUCache } from 'lru-cache';

// =============================================================================
// BOUNDED REQUEST COALESCING CACHE
// =============================================================================

/**
 * Cache entry for pending embedding requests.
 * Stores both the promise and timestamp for debugging/monitoring.
 */
interface CacheEntry {
  promise: Promise<{ embedding: number[] }>;
  timestamp: number;
}

/**
 * Bug #23 fix: Request coalescing cache with bounds to prevent memory exhaustion.
 *
 * Features:
 * - LRU eviction: Least recently used entries are removed first
 * - TTL: Entries expire after 60 seconds
 * - Size limit: Maximum 1000 concurrent embedding requests
 * - Automatic cleanup: TTL and eviction prevent unbounded growth
 *
 * Performance:
 * - Prevents cache stampede (multiple requests for same query)
 * - Bounded memory usage (~10MB max for 1000 entries)
 * - No memory leaks from failed promises
 */
const pendingEmbeddings = new LRUCache<string, CacheEntry>({
  max: 1000, // Max 1000 concurrent embedding requests
  ttl: 60_000, // 1 minute TTL (60 seconds)
  updateAgeOnGet: true, // LRU behavior - accessed entries stay fresh
  // Dispose is called when entries are evicted or expired
  dispose: (_value: CacheEntry, _key: string) => {
    // Log eviction for monitoring (can be removed in production)
    // Note: No cleanup needed as promise resolves/rejects independently
  },
});

/**
 * Get or create an embedding with bounded request coalescing.
 * Multiple concurrent requests for the same query share a single API call.
 *
 * Memory Safety:
 * - Cache is bounded by size (1000 entries) and TTL (60s)
 * - Failed promises are cleaned up via finally()
 * - LRU eviction prevents memory exhaustion
 */
async function getEmbeddingWithCoalescing(
  embeddingService: { embed(text: string): Promise<{ embedding: number[] }> },
  text: string
): Promise<{ embedding: number[] }> {
  // Check if there's already a pending request for this text
  const cached = pendingEmbeddings.get(text);
  if (cached) {
    return cached.promise;
  }

  // Create new request and store the promise
  const promise = embeddingService.embed(text).finally(() => {
    // Clean up after completion (success or failure)
    // This ensures failed promises don't persist in cache
    pendingEmbeddings.delete(text);
  });

  // Store in LRU cache with timestamp
  pendingEmbeddings.set(text, {
    promise,
    timestamp: Date.now(),
  });

  return promise;
}

/**
 * Extended pipeline context with semantic search results
 */
export interface SemanticStageContext extends PipelineContext {
  /**
   * The resolved search strategy for this query.
   * Determines whether semantic search is applied.
   */
  searchStrategy: SearchStrategy;

  /**
   * Query embedding generated by the embedding service.
   * Used for vector similarity calculations in scoring.
   */
  queryEmbedding?: number[];
}

/**
 * Async semantic stage - performs vector similarity search for semantic/hybrid modes
 *
 * Uses the embeddingService from dependencies if available.
 * Skips gracefully if service not available or strategy doesn't require semantic.
 *
 * Strategy selection:
 * 1. If searchStrategy is not 'semantic' or 'hybrid' -> pass through
 * 2. If no search query -> pass through
 * 3. If embedding service unavailable -> pass through with debug log
 * 4. Otherwise -> generate embedding and prepare semantic scores
 *
 * @param ctx Pipeline context with searchStrategy and search query
 * @returns Updated context with semanticScores and queryEmbedding
 */
export async function semanticStageAsync(ctx: PipelineContext): Promise<PipelineContext> {
  // Cast to access searchStrategy
  const semanticCtx = ctx as SemanticStageContext;
  const { search, deps } = ctx;
  const searchStrategy = semanticCtx.searchStrategy;

  // Skip if not semantic/hybrid mode or no search query
  if (searchStrategy !== 'semantic' && searchStrategy !== 'hybrid') {
    return ctx;
  }

  if (!search) {
    return ctx;
  }

  // Check if embedding service is available
  if (!deps.embeddingService || !deps.embeddingService.isAvailable()) {
    if (deps.logger && deps.perfLog) {
      deps.logger.debug({}, 'semantic stage skipped: embedding service unavailable');
    }
    return ctx;
  }

  try {
    const startMs = Date.now();
    const { types, limit } = ctx;

    // Check for HyDE embeddings from rewrite stage
    const searchQueries = (
      ctx as {
        searchQueries?: Array<{
          text: string;
          embedding?: number[];
          weight: number;
          source: string;
        }>;
      }
    ).searchQueries;
    const hydeQueries = searchQueries?.filter((q) => q.source === 'hyde' && q.embedding) ?? [];

    // 1. Generate query embedding (use first HyDE embedding if available, otherwise embed original)
    let queryEmbedding: number[];
    let embeddingsToSearch: Array<{ embedding: number[]; weight: number }> = [];

    if (hydeQueries.length > 0) {
      // Use HyDE embeddings for semantic search
      embeddingsToSearch = hydeQueries.map((q) => ({
        embedding: q.embedding!,
        weight: q.weight,
      }));
      // Use first HyDE embedding as the primary query embedding
      queryEmbedding = hydeQueries[0]!.embedding!;

      if (deps.logger && deps.perfLog) {
        deps.logger.debug(
          { hydeCount: hydeQueries.length },
          'semantic stage using HyDE embeddings'
        );
      }
    } else {
      // Fall back to embedding the original query
      // Bug #23 fix: Use request coalescing to prevent duplicate API calls
      const queryResult = await getEmbeddingWithCoalescing(deps.embeddingService, search);
      queryEmbedding = queryResult.embedding;
      embeddingsToSearch = [{ embedding: queryResult.embedding, weight: 1.0 }];
    }

    // 2. Search vector store for similar entries using all embeddings
    const semanticScores = new Map<string, number>();

    // Note: We check only if vectorService exists, not isAvailable().
    // isAvailable() returns false before initialization, but searchSimilar()
    // triggers lazy initialization via ensureInitialized(). The try/catch
    // below handles any initialization errors gracefully.
    if (deps.vectorService) {
      // Convert query types to entry types (e.g., 'tools' -> 'tool')
      const entryTypes = types.map((t) => {
        if (t === 'tools') return 'tool';
        if (t === 'guidelines') return 'guideline';
        if (t === 'experiences') return 'experience';
        return 'knowledge';
      });

      // Search with each embedding and combine results (max score wins)
      let totalResults = 0;
      // Bug #26 fix: Cap the candidate multiplier to prevent OOM on large limits
      // Fetch 3x candidates for scoring, but cap at 1000 to prevent memory issues
      const MAX_CANDIDATES = 1000;
      const candidateLimit = Math.min(limit * 3, MAX_CANDIDATES);

      for (const { embedding, weight } of embeddingsToSearch) {
        const results = await deps.vectorService.searchSimilar(
          embedding,
          entryTypes,
          candidateLimit
        );

        for (const r of results) {
          const weightedScore = r.score * weight;
          const existing = semanticScores.get(r.entryId) ?? 0;
          // Take the max score across all embeddings
          semanticScores.set(r.entryId, Math.max(existing, weightedScore));
        }
        totalResults += results.length;
      }

      if (deps.logger && deps.perfLog) {
        deps.logger.debug(
          {
            queryLength: search.length,
            embeddingDim: queryEmbedding.length,
            vectorResults: totalResults,
            uniqueResults: semanticScores.size,
            hydeCount: hydeQueries.length,
            timeMs: Date.now() - startMs,
          },
          'semantic stage completed with vector search'
        );
      }
    } else {
      // Vector service not configured - just store embedding for potential use in reranking
      if (deps.logger && deps.perfLog) {
        deps.logger.debug(
          {
            queryLength: search.length,
            embeddingDim: queryEmbedding.length,
            timeMs: Date.now() - startMs,
          },
          'semantic stage completed (no vector service configured)'
        );
      }
    }

    return {
      ...ctx,
      semanticScores,
      queryEmbedding,
    } as SemanticStageContext;
  } catch (error) {
    // Classify error and log appropriately
    if (deps.logger) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const errorCode = error instanceof AgentMemoryError ? error.code : undefined;

      // Expected errors (embeddings disabled) - log at debug level
      if (errorCode === ErrorCodes.EMBEDDING_DISABLED) {
        deps.logger.debug({ error: errorMessage }, 'semantic stage skipped: embeddings disabled');
      }
      // Transient errors (network, timeout) - log at warn level
      else if (
        errorCode === ErrorCodes.NETWORK_ERROR ||
        errorCode === ErrorCodes.TIMEOUT ||
        errorCode === ErrorCodes.SERVICE_UNAVAILABLE
      ) {
        deps.logger.warn(
          { error: errorMessage, errorCode, retriable: true },
          'semantic stage failed with transient error - continuing without semantic scores'
        );
      }
      // Unexpected errors - log at warn level with full context
      // Bug #197 fix: Include stack trace for unexpected errors to help diagnosis
      else {
        deps.logger.warn(
          {
            error: errorMessage,
            errorCode,
            errorType: error?.constructor?.name,
            search: search?.substring(0, 100), // Truncate for privacy
            stack:
              error instanceof Error ? error.stack?.split('\n').slice(0, 5).join('\n') : undefined,
          },
          'semantic stage failed unexpectedly - continuing without semantic scores'
        );
      }
    }
    return ctx;
  }
}
